#!/usr/bin/env python
# Copyright (C) 2024-2025 OVH SAS
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# ruff: noqa: E402
# isort: off
from oio.common.green import eventlet_monkey_patch

eventlet_monkey_patch()
# isort: on

import argparse
import json
import signal
import sys
import time
from collections import OrderedDict
from multiprocessing.queues import Empty
from os.path import exists, join

from oio.api.object_storage import ObjectStorageApi
from oio.common.configuration import read_conf
from oio.common.constants import (
    LIFECYCLE_PROPERTY_KEY,
    M2_PROP_LIFECYCLE_TIME_BYPASS,
    MULTIUPLOAD_SUFFIX,
)
from oio.common.easy_value import int_value
from oio.common.exceptions import NoSuchContainer, NoSuchObject, NotFound
from oio.common.green import GreenPool, LightQueue, sleep
from oio.common.kafka import KafkaSender
from oio.common.logger import get_logger
from oio.common.statsd import get_statsd
from oio.common.timestamp import Timestamp
from oio.common.utils import cid_from_name, get_hasher, request_id
from oio.container.lifecycle import lifecycle_backup_path
from oio.container.sharding import ContainerSharding
from oio.event.evob import EventTypes
from oio.lifecycle.metrics import (
    LifecycleAction,
    LifecycleMetricTracker,
    LifecycleStep,
    statsd_key,
)

CHECKPOINT_TOPIC_DEFAULT = "oio-lifecycle-checkpoint"


class Aborted(Exception):
    pass


class LifecycleConfigurationNotFound(Exception):
    pass


class LifecycleBackupContainerNotFound(Exception):
    pass


class CheckpointCollector:
    def __init__(self, conf, logger, feature, run_id, with_time_bypass=False):
        self._conf = conf
        self._logger = logger
        self._running = False
        self._feature_name = feature
        self._marker = None
        self._with_time_bypass = with_time_bypass

        # Configuration
        self._concurrency = int_value(self._conf.get("concurrency"), 100)
        self._topic = self._conf.get("topic", CHECKPOINT_TOPIC_DEFAULT)

        # Global bucket to save versions fof lifecyce configs
        self._lc_backup_account = self._conf.get(
            "lifecycle_configuration_backup_account"
        )
        if not self._lc_backup_account:
            raise ValueError(
                "Missing value for 'lifecycle_configuration_backup_account'"
            )
        self._lc_backup_bucket = self._conf.get("lifecycle_configuration_backup_bucket")
        if not self._lc_backup_bucket:
            raise ValueError(
                "Missing value for 'lifecycle_configuration_backup_bucket'"
            )

        # Threading
        self._pool = GreenPool(self._concurrency)
        self._result_queue = LightQueue()

        # Event producer
        self._kafka_producer = KafkaSender(
            self._conf.get("endpoint"),
            self._logger,
            app_conf=self._conf,
        )

        # Oio clients
        namespace = conf["namespace"]
        self._api = ObjectStorageApi(namespace, logger=logger)
        self._sharding_client = ContainerSharding(
            self._conf, logger=logger, pool_manager=self._api.container.pool_manager
        )

        # Metrics helper
        self._metrics = LifecycleMetricTracker(self._conf)

        # Statsd helpers
        self._statsd = get_statsd(self._conf)

        # Progress tracking
        self._progress_dir = self._conf.get("progress_dir", ".")
        self._run_id = run_id
        self._progress = OrderedDict()
        self._last_commited_marker = None
        self.has_error = False

        self._request_id = request_id(f"lc-{self._run_id}-")

        self._reload_progress()

    def _reload_progress(self):
        # load marker
        self._logger.info("Retrieve progress from file: '%s'", self._progress_file)
        if exists(self._progress_file):
            with open(self._progress_file, "r", encoding="utf-8") as progress_file:
                line = progress_file.readline()
                if not line:
                    return
                parts = line.split(";")
                if len(parts) != 2:
                    return
                account, bucket = parts
                self._logger.info(
                    "Reload marker account=%s, bucket=%s", account, bucket
                )
                self._last_commited_marker = self._marker = (account, bucket)

    @property
    def _progress_file(self):
        return join(self._progress_dir, f"checkpoint-collector.progress.{self._run_id}")

    @property
    def _error_file(self):
        return join(self._progress_dir, f"checkpoint-collector.error.{self._run_id}")

    def _make_payload(self, account, bucket, cid, shard_info):
        shard_info = shard_info or {}
        return json.dumps(
            {
                "event": EventTypes.LIFECYCLE_CHECKPOINT,
                "when": time.time(),
                "request_id": self._request_id,
                "data": {
                    "run_id": self._run_id,
                    "account": account,
                    "bucket": bucket,
                    "cid": shard_info.get("cid") or cid,
                    "root_cid": cid,
                    "bounds": {
                        "lower": shard_info.get("lower", ""),
                        "upper": shard_info.get("upper", ""),
                    },
                },
            }
        )

    def _increment_snapshot_counter(self, ctx, cid):
        if not ctx:
            return
        account, bucket = ctx
        with self._statsd.pipeline() as pipe:
            pipe.incr(
                statsd_key(
                    self._run_id, LifecycleStep.SUBMITTED, LifecycleAction.CHECKPOINT
                )
            )
        self._metrics.increment_counter(
            self._run_id,
            account,
            bucket,
            cid,
            LifecycleStep.SUBMITTED,
            LifecycleAction.CHECKPOINT,
        )

    def _process_container(self, account, container, ctx=None):
        cid = cid_from_name(account, container)
        # Produce event for root container
        self._produce_event(account, container, cid)
        self._increment_snapshot_counter(ctx, cid)

    def _produce_event(self, account, container, cid, shard_info=None):
        payload = self._make_payload(account, container, cid, shard_info)
        self._logger.info("Produce event %s", container)
        self._kafka_producer.send(self._topic, payload, flush=True)

    def _upload_configuration(self, account, bucket, props):
        lifecycle_config = props["properties"].get(LIFECYCLE_PROPERTY_KEY)
        if not lifecycle_config:
            self._logger.error(
                "Failed to get lifecycle config for account %s, bucket %s",
                account,
                bucket,
            )
            raise LifecycleConfigurationNotFound(
                f"No configuration found for bucket: {bucket}, account: {account}"
            )
        try:
            obj_name = lifecycle_backup_path(account, bucket)
            should_upload = True
            try:
                object_props = self._api.object_get_properties(
                    self._lc_backup_account,
                    self._lc_backup_bucket,
                    obj_name,
                    reqid=self._request_id,
                )
                # Compute lifecycle configuration hash
                hasher = get_hasher("md5")
                hasher.update(lifecycle_config.encode("utf-8"))
                checksum = hasher.hexdigest().upper()

                if checksum == object_props.get("hash"):
                    should_upload = False

            except (NotFound, NoSuchObject):
                self._logger.debug(
                    "No previous lifecycle configuration found for %s", obj_name
                )

            if should_upload:
                self._api.object_create(
                    self._lc_backup_account,
                    self._lc_backup_bucket,
                    obj_name=obj_name,
                    data=lifecycle_config,
                    reqid=self._request_id,
                )
        except NoSuchContainer as exc:
            self._logger.error(
                "Failed to access lifecycle configuration backup container, "
                "account: %s, container: %s",
                self._lc_backup_account,
                self._lc_backup_bucket,
            )
            raise LifecycleBackupContainerNotFound() from exc

    def _process_bucket(self, account, bucket):
        error = None
        self._progress[(account, bucket)] = None
        if not self._metrics.is_bucket_triggered(self._run_id, account, bucket):
            ctx = (account, bucket)
            self._logger.info("Processing %s %s", account, bucket)
            try:
                for ct_suffix in ("", MULTIUPLOAD_SUFFIX):
                    container = f"{bucket}{ct_suffix}"
                    try:
                        if self._with_time_bypass and not ct_suffix:
                            props = self._api.container_get_properties(
                                account,
                                container,
                                reqid=self._request_id,
                            )
                            if props.get(M2_PROP_LIFECYCLE_TIME_BYPASS, False):
                                break

                        self._logger.debug(
                            "Get info on container: acct=%s, ref=%s", account, bucket
                        )
                        props = self._api.container_show(
                            account,
                            container,
                            reqid=self._request_id,
                        )
                        if not ct_suffix:
                            # This is the main container
                            self._upload_configuration(account, bucket, props)

                        self._process_container(account, container, ctx=ctx)
                    except NoSuchContainer:
                        self._logger.debug(
                            "Container not found account: %s, container: %s",
                            account,
                            container,
                        )
                        continue
                self._metrics.mark_bucket_as_triggered(self._run_id, account, bucket)
            except Exception as exc:
                self._logger.error(
                    "Failed to process bucket %s, reason: %s", bucket, exc
                )
                error = exc
        self._result_queue.put((account, bucket, error))

    def _fetch_buckets(self, marker=None):
        if isinstance(marker, tuple):
            marker = "|".join(marker)

        while True:
            resp = self._api.bucket.buckets_list_by_feature(
                self._feature_name,
                marker=marker,
                limit=100,
                reqid=self._request_id,
            )
            for entry in resp.get("buckets", []):
                yield entry["account"], entry["bucket"]
            if not resp.get("truncated", False):
                break
            marker = resp.get("next_marker")
            if not marker:
                break

    def _compute_progress(self, account, bucket, status):
        self._progress[(account, bucket)] = status
        marker = None

        while True:
            key = None
            value = None
            if self._progress:
                # Retrieve first item
                key, value = next(iter(self._progress.items()))
            if value is None:
                break
            _, _ = self._progress.popitem(last=False)
            marker = key

        if marker is not None:
            self._last_commited_marker = marker

    def _fetch_progression(self):
        last_marker = None
        with open(self._progress_file, "w", encoding="utf-8") as progress_file:
            with open(self._error_file, "w", encoding="utf-8") as error_file:
                while True:
                    try:
                        result = self._result_queue.get(timeout=1)
                    except Empty:
                        if not self._progress:
                            break
                    account, bucket, error = result
                    if error:
                        error_file.write(f"{account};{bucket};{error}")
                        self.has_error = True
                    self._compute_progress(account, bucket, error or True)
                    if (
                        self._last_commited_marker
                        and self._last_commited_marker != last_marker
                    ):
                        self._logger.debug(
                            "Update progress file %s", self._last_commited_marker
                        )
                        account, bucket = self._last_commited_marker
                        progress_file.truncate(0)
                        progress_file.seek(0)
                        progress_file.write(f"{account};{bucket}\n")
                        last_marker = self._last_commited_marker

    def __stop(self):
        self._logger.info("Stopping")
        self._running = False

    def run(self):
        """ """
        # Install signal handlers
        signal.signal(signal.SIGINT, lambda _sig, _stack: self.__stop())
        signal.signal(signal.SIGTERM, lambda _sig, _stack: self.__stop())
        self._running = True

        task_progression = self._pool.spawn(self._fetch_progression)

        tasks = [task_progression]

        def cancel_pending_tasks():
            self._logger.warning("Aborting pending tasks")
            for task in tasks:
                task.cancel()

        self._logger.info(
            "Listing bucket with feature '%s' activated", self._feature_name
        )
        has_process_bucket = bool(self._marker)
        for account, bucket in self._fetch_buckets(self._marker):
            if not self._running:
                cancel_pending_tasks()
                break
            has_process_bucket = True
            task = self._pool.spawn(self._process_bucket, account, bucket)
            tasks.append(task)
        if not has_process_bucket:
            self._logger.warning(
                "No buckets found with feature '%s'. Ensure the feature is tracked"
                "(features_whitelist)",
                self._feature_name,
            )

        while (self._pool.running() + self._pool.waiting()) > 0:
            if not self._running:
                cancel_pending_tasks()
                break
            sleep(1)
        # Let all threads end
        self._pool.waitall()

        self._kafka_producer.close()


def make_arg_parser():
    descr = """
    Generate events to create checkpoints for lifecycle enabled containers
    """
    parser = argparse.ArgumentParser(description=descr)

    parser.add_argument(
        "--verbose", "-v", action="store_true", help="More verbose output"
    )
    parser.add_argument(
        "--run-id", help="Run identifier", default=str(Timestamp().timestamp)
    )
    parser.add_argument(
        "--only-with-time-bypass",
        help="Trigger lifecycle pass only for bucket with time bypass enabled",
        default=False,
        action="store_true",
    )
    parser.add_argument("configuration", help="Path to the legacy configuration file")
    return parser


def main():
    args = make_arg_parser().parse_args()
    conf = read_conf(args.configuration, "checkpoint-collector")
    logger = get_logger(conf, verbose=args.verbose)

    collector = CheckpointCollector(conf, logger, "lifecycle", args.run_id)
    collector.run()
    return 1 if collector.has_error else 0


if __name__ == "__main__":
    sys.exit(main())
