#!/usr/bin/env python
# Copyright (C) 2024-2025 OVH SAS
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# ruff: noqa: E402
# isort: off
from oio.common.green import eventlet_monkey_patch

eventlet_monkey_patch()
# isort: on

import argparse
import json
import signal
import sys
import time
from collections import Counter

from oio.api.object_storage import ObjectStorageApi
from oio.common.configuration import read_conf
from oio.common.constants import (
    LIFECYCLE_PROPERTY_KEY,
    M2_PROP_LIFECYCLE_TIME_BYPASS,
    MULTIUPLOAD_SUFFIX,
)
from oio.common.easy_value import boolean_value, int_value
from oio.common.exceptions import NoSuchContainer, NoSuchObject, NotFound
from oio.common.green import GreenPool, LightQueue
from oio.common.kafka import DEFAULT_LIFECYCLE_CHECKPOINT_TOPIC, KafkaSender
from oio.common.logger import (
    OioAccessLog,
    get_oio_log_context,
    get_oio_logger,
    redirect_stdio,
)
from oio.common.statsd import StatsdTiming, get_statsd
from oio.common.timestamp import Timestamp
from oio.common.utils import cid_from_name, get_hasher, initialize_coverage, request_id
from oio.container.lifecycle import lifecycle_backup_path
from oio.container.sharding import ContainerSharding
from oio.event.evob import EventTypes
from oio.lifecycle.metrics import (
    LifecycleAction,
    LifecycleMetricTracker,
    LifecycleStep,
    statsd_key,
)


class BucketAlreadyProcessed(Exception):
    pass


class BucketTimeBypassDisabled(Exception):
    pass


class LifecycleConfigurationNotFound(Exception):
    pass


class LifecycleBackupContainerNotFound(Exception):
    pass


class CheckpointCollector:
    status_codes = (200, 208, 404, 412, 500)

    def __init__(self, conf, logger, feature, run_id, with_time_bypass=False):
        self._conf = conf
        self._logger = logger
        self._running = False
        self._feature_name = feature
        self._marker = None
        self._with_time_bypass = with_time_bypass

        # Configuration
        self._concurrency = int_value(self._conf.get("concurrency"), 100)
        self._topic = self._conf.get("topic", DEFAULT_LIFECYCLE_CHECKPOINT_TOPIC)

        # Global bucket to save versions fof lifecyce configs
        self._lc_backup_account = self._conf.get(
            "lifecycle_configuration_backup_account"
        )
        if not self._lc_backup_account:
            raise ValueError(
                "Missing value for 'lifecycle_configuration_backup_account'"
            )
        self._lc_backup_bucket = self._conf.get("lifecycle_configuration_backup_bucket")
        if not self._lc_backup_bucket:
            raise ValueError(
                "Missing value for 'lifecycle_configuration_backup_bucket'"
            )
        self._policy = self._conf.get("policy", None)

        # Threading
        self._pool = GreenPool(self._concurrency)
        self._result_queue = LightQueue()

        # Event producer
        self._kafka_producer = KafkaSender(
            self._conf.get("endpoint"),
            self._logger,
            app_conf=self._conf,
        )

        # Oio clients
        namespace = conf["namespace"]
        self._api = ObjectStorageApi(namespace, logger=logger)
        self._sharding_client = ContainerSharding(
            self._conf, logger=logger, pool_manager=self._api.container.pool_manager
        )

        # Metrics helper
        self._metrics = LifecycleMetricTracker(self._conf, logger=self._logger)
        self._process_stats = Counter()

        # Statsd helpers
        self._statsd = get_statsd(self._conf)

        self._run_id = run_id
        self._last_commited_marker = None
        self.has_error = False

        self._request_id = request_id(f"lc-{self._run_id}-")

    def _make_payload(self, account, bucket, cid, shard_info):
        shard_info = shard_info or {}
        return json.dumps(
            {
                "event": EventTypes.LIFECYCLE_CHECKPOINT,
                "when": time.time(),
                "request_id": self._request_id,
                "data": {
                    "run_id": self._run_id,
                    "account": account,
                    "bucket": bucket,
                    "cid": shard_info.get("cid") or cid,
                    "root_cid": cid,
                    "bounds": {
                        "lower": shard_info.get("lower", ""),
                        "upper": shard_info.get("upper", ""),
                    },
                },
            }
        )

    def _increment_snapshot_counter(self, account, bucket, cid, step):
        with self._statsd.pipeline() as pipe:
            pipe.incr(statsd_key(self._run_id, step, LifecycleAction.CHECKPOINT))
        self._metrics.increment_counter(
            self._run_id,
            account,
            bucket,
            cid,
            step,
            LifecycleAction.CHECKPOINT,
        )

    def _process_container(self, account, bucket, container, cid):
        try:
            self._logger.debug("Get info on container")
            props = self._api.container_get_properties(
                account,
                container,
                reqid=self._request_id,
            )
            if not container.endswith(MULTIUPLOAD_SUFFIX):
                # Processing main container
                if self._with_time_bypass and not boolean_value(
                    props.get(M2_PROP_LIFECYCLE_TIME_BYPASS), False
                ):
                    raise BucketTimeBypassDisabled()
                self._upload_configuration(account, bucket, props)

            # Produce event for root container
            self._produce_event(account, bucket, container, cid)
            self._increment_snapshot_counter(
                account, bucket, cid, LifecycleStep.SUBMITTED
            )
        except NoSuchContainer:
            self._logger.debug("Container not found")
            if not container.endswith(MULTIUPLOAD_SUFFIX):
                raise

    def _produce_event(self, account, bucket, container, cid, shard_info=None):
        payload = self._make_payload(account, container, cid, shard_info)
        self._logger.debug("Produce event")
        self._kafka_producer.send(self._topic, payload, flush=True)

    def _upload_configuration(self, account, bucket, props):
        version = None
        lifecycle_config = props["properties"].get(LIFECYCLE_PROPERTY_KEY)
        if not lifecycle_config:
            self._logger.error(
                "Failed to get lifecycle config for account %s, bucket %s",
                account,
                bucket,
            )
            raise LifecycleConfigurationNotFound(
                f"No configuration found for bucket: {bucket}, account: {account}"
            )
        try:
            obj_name = lifecycle_backup_path(account, bucket)
            should_upload = True
            try:
                object_props = self._api.object_get_properties(
                    self._lc_backup_account,
                    self._lc_backup_bucket,
                    obj_name,
                    reqid=self._request_id,
                )
                version = object_props["version"]
                # Compute lifecycle configuration hash
                hasher = get_hasher("md5")
                hasher.update(lifecycle_config.encode("utf-8"))
                checksum = hasher.hexdigest().upper()

                if checksum == object_props.get("hash"):
                    should_upload = False

            except (NotFound, NoSuchObject):
                self._logger.debug(
                    "No previous lifecycle configuration found for %s", obj_name
                )

            if should_upload:
                _, _, _, props = self._api.object_create_ext(
                    self._lc_backup_account,
                    self._lc_backup_bucket,
                    obj_name=obj_name,
                    data=lifecycle_config,
                    policy=self._policy,
                    reqid=self._request_id,
                )
                version = props["version"]
        except NoSuchContainer as exc:
            self._logger.error(
                "Failed to access lifecycle configuration backup container, "
                "account: %s, container: %s",
                self._lc_backup_account,
                self._lc_backup_bucket,
            )
            raise LifecycleBackupContainerNotFound() from exc
        return version

    def _process_bucket(self, account, bucket):
        code = 200
        try:
            with OioAccessLog(
                self._logger,
                account=account,
                bucket=bucket,
                request_id=self._request_id,
                run_id=self._run_id,
            ) as access:
                with StatsdTiming(
                    self._statsd,
                    "openio.lifecycle.checkpoint-collector.{code}.duration",
                ) as st:
                    if self._metrics.is_bucket_triggered(self._run_id, account, bucket):
                        code = access.status = st.code = 208
                        raise BucketAlreadyProcessed()
                    try:
                        self._logger.debug("Processing")
                        for ct_suffix in ("", MULTIUPLOAD_SUFFIX):
                            container = f"{bucket}{ct_suffix}"
                            cid = cid_from_name(account, container)
                            with get_oio_log_context(cid=cid, container=container):
                                try:
                                    self._process_container(
                                        account, bucket, container, cid
                                    )
                                except NoSuchContainer:
                                    self._logger.debug("Container not found")
                                    code = access.status = st.code = 404
                                    raise
                    except BucketTimeBypassDisabled:
                        code = access.status = st.code = 412
                        raise
                self._metrics.mark_bucket_as_triggered(self._run_id, account, bucket)
        except BucketAlreadyProcessed:
            self._logger.debug("Bucket already processed")
        except Exception as exc:
            self._logger.error("Failed to process bucket, reason: %s", exc)
            cid = cid_from_name(account, bucket)
            self._increment_snapshot_counter(
                account, bucket, cid, LifecycleStep.SUBMIT_ERROR
            )
            if code == 200:
                code = 500
        return code

    def _fetch_buckets(self):
        marker = None
        while True:
            resp = self._api.bucket.buckets_list_by_feature(
                self._feature_name,
                marker=marker,
                limit=100,
                reqid=self._request_id,
            )
            for entry in resp.get("buckets", []):
                if not self._running:
                    return
                yield entry["account"], entry["bucket"]
            if not resp.get("truncated", False):
                break
            marker = resp.get("next_marker")
            if not marker:
                break

    def __stop(self):
        self._logger.debug("Stopping")
        self._running = False

    def run(self):
        with OioAccessLog(
            self._logger,
            request_id=self._request_id,
            run_id=self._run_id,
        ) as access:
            with StatsdTiming(
                self._statsd,
                "openio.lifecycle.checkpoint-collector.run.{code}.duration",
            ) as st:
                # Install signal handlers
                signal.signal(signal.SIGINT, lambda _sig, _stack: self.__stop())
                signal.signal(signal.SIGTERM, lambda _sig, _stack: self.__stop())
                self._running = True

                metrics = Counter({c: 0 for c in self.status_codes})
                for code in self._pool.imap(
                    lambda args: self._process_bucket(*args), self._fetch_buckets()
                ):
                    if code not in metrics:
                        self._logger.warning("Code %s is not recognized", code)
                    metrics[code] += 1
                    self.has_error = self.has_error or code >= 500

                for code, value in metrics.items():
                    self._statsd.gauge(
                        f"openio.lifecycle.checkpoint-collector.run.buckets.{code}",
                        value,
                    )

                self._kafka_producer.close()

                if self.has_error:
                    access.status = st.code = 500


def make_arg_parser():
    descr = """
    Generate events to create checkpoints for lifecycle enabled containers
    """
    parser = argparse.ArgumentParser(description=descr)

    parser.add_argument(
        "--verbose", "-v", action="store_true", help="More verbose output"
    )
    parser.add_argument(
        "--run-id", help="Run identifier", default=str(Timestamp().timestamp)
    )
    parser.add_argument(
        "--only-with-time-bypass",
        help="Trigger lifecycle pass only for bucket with time bypass enabled",
        default=False,
        action="store_true",
    )
    parser.add_argument(
        "--coverage",
        action="store_true",
        help=(
            "Import code coverage lib, and measure coverage "
            "if COVERAGE_PROCESS_START is also set."
        ),
    )
    parser.add_argument("configuration", help="Path to the legacy configuration file")
    return parser


def main():
    args = make_arg_parser().parse_args()
    conf = read_conf(args.configuration, "checkpoint-collector")
    logger = get_oio_logger(conf, verbose=args.verbose)
    redirect_stdio(logger)

    if args.coverage:
        initialize_coverage(logger, "checkpoint-collector")

    with get_oio_log_context(run_id=args.run_id):
        logger.info("Starting")
        collector = CheckpointCollector(
            conf,
            logger,
            "lifecycle",
            args.run_id,
            with_time_bypass=args.only_with_time_bypass,
        )
        collector.run()
    return 1 if collector.has_error else 0


if __name__ == "__main__":
    sys.exit(main())
